Based on the paper "A Deep Dynamic Latent Block Model for the Co-clustering of Zero-Inflated Data Matrices" (Zip-dLBM), here is a detailed, context-rich README designed for an autonomous agent or developer to implement the model.Zip-dLBM: Zero-Inflated Dynamic Latent Block Model1. Project OverviewZip-dLBM is a probabilistic generative model designed for the simultaneous clustering (co-clustering) of rows and columns in time-evolving, high-dimensional, and sparse (zero-inflated) data matrices.The model combines:Latent Block Modeling (LBM): To group observations and features.Zero-Inflated Distributions: Specifically Zero-Inflated Poisson (ZIP) to handle sparsity.Dynamic Systems (ODEs): To model the evolution of cluster proportions and sparsity over time.Deep Learning: Neural networks are used to parameterize the ordinary differential equations governing the dynamics.Reference: Marchello, G., Corneli, M., & Bouveyron, C. (2025). A Deep Dynamic Latent Block Model for the Co-clustering of Zero-Inflated Data Matrices. HAL Id: hal-04150292.2. Mathematical Generative Model2.1 Notation and Dimensions$T$: Number of discrete time steps.$N$: Number of rows (observations).$M$: Number of columns (features).$X(t)$: The $N \times M$ incidence matrix at time $t$.$Q$: Number of row clusters (latent).$L$: Number of column clusters (latent).2.2 Latent VariablesThe model relies on three sets of latent variables:Row Clustering $Z(t)$: $Z_{i}(t) \sim \mathcal{M}(1, \alpha(t))$.$Z_{iq}(t) = 1$ if row $i$ belongs to cluster $q$ at time $t$.Column Clustering $W(t)$: $W_{j}(t) \sim \mathcal{M}(1, \beta(t))$.$W_{j\ell}(t) = 1$ if column $j$ belongs to cluster $\ell$ at time $t$.Sparsity Indicator $A(t)$: $A_{ij}(t) \sim \mathcal{B}(\pi(t))$.$A_{ij}(t) = 1$ implies the entry is a structural zero.$A_{ij}(t) = 0$ implies the entry follows the count distribution.2.3 The Zero-Inflated ProcessThe observations $X_{ij}(t)$ are generated via a mixture of a Dirac mass at zero and a block-dependent Poisson distribution:$$X_{ij}(t) | Z_i(t), W_j(t) \sim \text{ZIP}(\Lambda_{Z_i(t), W_j(t)}; \pi(t))$$This decomposes to:If $A_{ij}(t) = 1$: $X_{ij}(t) = 0$ (Structural Zero).If $A_{ij}(t) = 0$: $X_{ij}(t) \sim \mathcal{P}(\Lambda_{q\ell})$ where $Z_{iq}=1, W_{j\ell}=1$.Parameters:$\Lambda \in \mathbb{R}^{Q \times L}$: Block-dependent Poisson intensity matrix (assumed static over blocks, though block memberships change).$\pi(t) \in [0, 1]$: Sparsity probability at time $t$.2.4 Dynamic Evolution (ODEs & Neural Networks)The mixing proportions ($\alpha(t), \beta(t)$) and sparsity ($\pi(t)$) evolve according to ODEs modeled by Neural Networks:$$\frac{d}{dt} a(t) = f_Z(a(t)), \quad \frac{d}{dt} b(t) = f_W(b(t)), \quad \frac{d}{dt} c(t) = f_A(c(t))$$Where the probabilities are obtained via link functions:$\alpha_q(t) = \text{softmax}(a_q(t))$$\beta_\ell(t) = \text{softmax}(b_\ell(t))$$\pi(t) = \text{sigmoid}(c(t))$Implementation Note: The functions $f_Z, f_W, f_A$ are approximated using fully connected neural networks.3. Inference Algorithm (VEM-SGD)The inference cannot use standard EM due to the complex dependency structure. A Variational EM (VEM) algorithm combined with Stochastic Gradient Descent (SGD) is used.Objective: Maximize the Evidence Lower Bound (ELBO) $\mathcal{L}(q, \theta)$.3.1 Variational DistributionsWe introduce variational distributions $q(\cdot)$ to approximate the true posterior:$q(A_{ij}(t)) \sim \mathcal{B}(\delta_{ij}(t))$$q(Z_{i}(t)) \sim \mathcal{M}(1, \tau_{i}(t))$$q(W_{j}(t)) \sim \mathcal{M}(1, \eta_{j}(t))$3.2 Algorithm FlowStep A: VE-Step (Variational Expectation)Update the variational parameters while keeping model parameters $\theta$ fixed.1. Update Sparsity Probability $\delta_{ij}(t)$:$$\delta_{ij}(t) = \frac{\exp(R_{ij}(t))}{1 + \exp(R_{ij}(t))}$$Where $R_{ij}(t)$ is the log-odds (see Appendix A in paper for full derivation involving $\Lambda, \tau, \eta$).Note: If $X_{ij}(t) > 0$, then strictly $\delta_{ij}(t) = 0$.2. Update Row Membership $\tau_{iq}(t)$:$$\tau_{iq}(t) \propto \alpha_q(t) \exp\left( \sum_{j, \ell} (1 - \delta_{ij}(t)) \eta_{j\ell}(t) [X_{ij}(t) \log \Lambda_{q\ell} - \Lambda_{q\ell}] \right)$$Normalize so $\sum_q \tau_{iq}(t) = 1$.3. Update Column Membership $\eta_{j\ell}(t)$:$$\eta_{j\ell}(t) \propto \beta_\ell(t) \exp\left( \sum_{i, q} (1 - \delta_{ij}(t)) \tau_{iq}(t) [X_{ij}(t) \log \Lambda_{q\ell} - \Lambda_{q\ell}] \right)$$Normalize so $\sum_\ell \eta_{j\ell}(t) = 1$.Step B: M-Step (Maximization)Update model parameters $\theta = \{\Lambda, \alpha, \beta, \pi\}$ maximizing the ELBO.1. Update Poisson Matrix $\Lambda$ (Closed Form):$$\hat{\Lambda}_{q\ell} = \frac{\sum_{i,j,t} \tau_{iq}(t) \eta_{j\ell}(t) (X_{ij}(t) [1 - \delta_{ij}(t)])}{\sum_{i,j,t} \tau_{iq}(t) \eta_{j\ell}(t) (1 - \delta_{ij}(t))}$$Logic: Weighted average of observations assigned to block $(q, \ell)$, adjusted for the probability of being non-zero.2. Update Dynamics $\alpha, \beta, \pi$ (Neural Network Optimization):The parameters $\alpha(t), \beta(t), \pi(t)$ are outputs of the ODE solver (Euler method) driven by the Neural Networks.Loss Function: The negative part of the ELBO depending on these parameters.Optimization: Use PyTorch/TensorFlow automatic differentiation to compute gradients $\nabla \mathcal{L}$ with respect to NN weights $\omega_A, \omega_Z, \omega_W$.Optimizer: ADAM.4. Implementation Details4.1 Neural Network ArchitectureBased on Appendix F:Type: Fully connected feed-forward networks (MLP).Structure:Input layer.Hidden Layers: 2 layers.Width: 200 neurons per hidden layer.Activation: Not explicitly specified in text, but standard practices (ReLU or Tanh) apply for ODE approximation.Output: Dimensions matching $Q$ (for $\alpha$), $L$ (for $\beta$), and 1 (for $\pi$).4.2 Training HyperparametersEpochs (M-Step): 2000 epochs per VEM iteration for the neural network optimization.Learning Rate: $\gamma = 1e-4$.Batching: Full-batch training is required. Mini-batching is not suitable due to the interdependence of rows and columns in co-clustering.ODE Solver: Euler scheme with step size $\Delta = 1$.$a(t+1) = a(t) + \text{NN}_Z(a(t))$.4.3 Computational EnvironmentFramework: PyTorch (recommended for Autograd).Hardware: Capable of running on standard CPU (referenced MacBook Pro i7), but GPU is recommended for the NN backpropagation steps.5. Initialization and Model Selection StrategyThe paper proposes a specific "Cascade" strategy to avoid testing all pairs of $(Q, L)$ and to handle initialization robustly.Step 1: Slice Selection & Static ICLSelect the first time slice $X(t_0)$.Run static Zip-dLBM for a grid of $(q, \ell)$.Select $(Q_{init}, L_{init})$ maximizing the Integrated Completed Likelihood (ICL) criterion.Step 2: Cascade InitializationUse parameters from $t$ to initialize static clustering at $t+1$.Propagate $\hat{\theta}(t)$ forward through $t=1 \dots T$.Step 3: Over-allocation (The "Blessing" of Deep NNs)When running the full dynamic model, set $Q_{max} > Q_{init}$ and $L_{max} > L_{init}$ (e.g., if ICL suggests 6, run with 10).Initialize the "active" clusters using the cascade results.Initialize "extra" clusters with near-zero proportions.Result: The Deep Neural Networks will automatically activate empty clusters if the dynamics require them, or keep them empty if unnecessary. This avoids exhaustive grid search.6. Input Data FormatThe agent should expect data in the following tensor format:Tensor: 3D Tensor X of shape (N, M, T).Type: Integer (Count data) or Float (if normalized, though Poisson implies integers).Sparsity: Data is expected to contain a high percentage of zeros (80-90% in experiments).